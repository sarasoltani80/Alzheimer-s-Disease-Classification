{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5170962,"sourceType":"datasetVersion","datasetId":3005457}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input/alzheimers-adni","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport keras \nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import SMOTE\nfrom tensorflow.keras import layers, models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\nimages = []\nlabels = []\n\ntrain_path = \"/kaggle/input/alzheimers-adni/Alzheimers-ADNI/train\"\n\nfor subfolder in tqdm(os.listdir(train_path)):\n    subfolder_path = os.path.join(train_path, subfolder)\n    \n    \n    if os.path.isfile(subfolder_path):\n        images.append(subfolder_path)\n        labels.append(subfolder)  \n        continue\n    \n    \n    for folder in os.listdir(subfolder_path):\n        subfolder_path2 = os.path.join(subfolder_path, folder)\n\n        if os.path.isfile(subfolder_path2):  \n           \n            images.append(subfolder_path2)\n            labels.append(subfolder)  \n            continue\n\n        \n        if os.path.isdir(subfolder_path2):\n            for image_filename in os.listdir(subfolder_path2):\n                image_path = os.path.join(subfolder_path2, image_filename)\n                if os.path.isfile(image_path):  \n                    images.append(image_path)\n                    labels.append(folder)  \n\ntrain_df = pd.DataFrame({'image': images, 'label': labels})\ntrain_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\n\nimages = []\nlabels = []\n\ntrain_path = \"/kaggle/input/alzheimers-adni/Alzheimers-ADNI/test\"\n\nfor subfolder in tqdm(os.listdir(train_path)):\n    subfolder_path = os.path.join(train_path, subfolder)\n    \n    \n    if os.path.isfile(subfolder_path):\n        images.append(subfolder_path)\n        labels.append(subfolder)  \n        continue\n    \n    \n    for folder in os.listdir(subfolder_path):\n        subfolder_path2 = os.path.join(subfolder_path, folder)\n\n        if os.path.isfile(subfolder_path2):  \n            \n            images.append(subfolder_path2)\n            labels.append(subfolder)  \n            continue\n\n        \n        if os.path.isdir(subfolder_path2):\n            for image_filename in os.listdir(subfolder_path2):\n                image_path = os.path.join(subfolder_path2, image_filename)\n                if os.path.isfile(image_path):  \n                    images.append(image_path)\n                    labels.append(folder)  \n\n\ntest_df = pd.DataFrame({'image': images, 'label': labels})\n\n\ntest_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# split into 50% test and 50% validation\nsplit_index = len(test_df) // 2\nvalidation_df = test_df.iloc[:split_index].reset_index(drop=True)\ntest_df = test_df.iloc[split_index:].reset_index(drop=True)\n\n\nprint(f\"Test Set: {len(test_df)} samples\")\nprint(f\"Validation Set: {len(validation_df)} samples\")\n\nprint(test_df)\nprint(validation_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert categorical labels to integers\nlabel_map = {'Final AD JPEG': 0, 'Final CN JPEG': 1, 'Final EMCI JPEG': 2, 'Final LMCI JPEG': 3, 'Final MCI JPEG': 4}\ntrain_df['label'] = train_df['label'].map(label_map)\nvalidation_df['label'] = validation_df['label'].map(label_map)\ntest_df['label'] = test_df['label'].map(label_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['label'] = train_df['label'].astype(int)\nvalidation_df['label'] = validation_df['label'].astype(int)\ntest_df['label'] = test_df['label'].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_counts = validation_df['label'].value_counts()\n\n# Print exact counts\nprint(\"Exact counts for each class:\")\nprint(class_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_counts = train_df['label'].value_counts()\nprint(\"Exact counts for each class:\")\nprint(class_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Using Continual Learning Approach**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\n\nclass BrainDataset(Dataset):\n    def __init__(self, dataframe, image_size=(224, 224), transform=None):\n        self.dataframe = dataframe\n        self.image_size = image_size\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['image']\n        label = self.dataframe.iloc[idx]['label']\n\n        \n        image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n        image = image.resize(self.image_size)\n\n        if self.transform:\n            image = self.transform(image)\n \n\n        # Convert label to a tensor\n        label = torch.tensor(label, dtype=torch.long)\n\n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to 224x224\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = BrainDataset(train_df, transform=transform)\nval_dataset = BrainDataset(validation_df, transform=transform)\ntest_dataset = BrainDataset(test_df, transform=transform)\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Define the 3x3 convolution function\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n# Define the BasicBlock\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, cfg=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, cfg, stride)\n        self.bn1 = nn.BatchNorm2d(cfg)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(cfg, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu(out)\n        return out\n\n# Define the ResNet model for brain images\nclass ResNet_Brain(nn.Module):\n    def __init__(self, block, layers, cfg=None, num_classes=5):\n        super(ResNet_Brain, self).__init__()\n        self.inplanes = 64  # Increased initial channels for brain images\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  \n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        # Residual layers\n        self.layer1 = self._make_layer(block, 64, layers[0], cfg=cfg[0:layers[0]])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, cfg=cfg[layers[0]:layers[0] + layers[1]])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, cfg=cfg[layers[0] + layers[1]:layers[0] + layers[1] + layers[2]])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, cfg=cfg[layers[0] + layers[1] + layers[2]:layers[0] + layers[1] + layers[2] + layers[3]])\n        \n        # Global average pooling and fully connected layer\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling for variable input sizes\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1, cfg=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, cfg=cfg[0]))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cfg=cfg[i]))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n# Define a function to create the ResNet model for brain images\ndef resnet18_brain(cfg=None, num_classes=5):\n    return ResNet_Brain(BasicBlock, [2, 2, 2, 2], cfg=cfg, num_classes=num_classes)\n\n\ncfg = [64] * 8 + [128] * 8 + [256] * 8 + [512] * 8  # Example configuration\nmodel = resnet18_brain(cfg=cfg, num_classes=5)\nprint(model)\n\n\n#input_tensor = torch.randn(1, 3, 224, 224)  \n#output = model(input_tensor)\n#print(output.shape)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()  # Loss function for classification\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# Hyperparameters\nnum_epochs = 50\npatience = 5  # Number of epochs to wait for improvement before stopping\nbest_val_loss = np.inf  # Initialize best validation loss to infinity\nepochs_without_improvement = 0  # Counter for epochs without improvement\n\n# Early stopping function\ndef early_stopping(val_loss, patience, epochs_without_improvement, best_val_loss):\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        epochs_without_improvement = 0\n        # Save the best model\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement >= patience:\n            print(f\"Early stopping triggered after {epochs_without_improvement} epochs without improvement.\")\n            return True, best_val_loss, epochs_without_improvement\n    return False, best_val_loss, epochs_without_improvement\n\n# Training loop with early stopping\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Training phase\n    for images, labels in train_loader:\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Compute accuracy\n        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    train_accuracy = 100 * correct / total\n    train_loss = running_loss / len(train_loader)\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            outputs = model(images)\n            val_loss = criterion(outputs, labels)\n            val_running_loss += val_loss.item()\n\n            # Compute accuracy\n            _, predicted = torch.max(outputs, 1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_accuracy = 100 * val_correct / val_total\n    val_loss = val_running_loss / len(val_loader)\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    # Early stopping check\n    stop_training, best_val_loss, epochs_without_improvement = early_stopping(\n        val_loss, patience, epochs_without_improvement, best_val_loss\n    )\n    if stop_training:\n        break\n\nprint(\"Training complete. Best model saved to 'best_model.pth'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()  # set model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        \n\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Validation Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()  # set model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        \n\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**preparing data for other two models**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\n\nimage_size = (224, 224)\nbatch_size = 32\n\n# function to add Gaussian noise\ndef add_gaussian_noise(image):\n    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.05, dtype=tf.float32)\n    return tf.clip_by_value(image + noise, 0.0, 1.0)  # Ensures values remain between [0,1]\n\n# data augmentation for training set\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=lambda x: add_gaussian_noise(tf.keras.applications.vgg16.preprocess_input(x)),\n    rescale=1./255,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2]\n)\n\n\ntest_val_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n    rescale=1./255\n)\n\n# Train dataset with augmentation\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True\n)\n\n# Validation dataset (no augmentation)\nval_generator = test_val_datagen.flow_from_dataframe(\n    validation_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True\n)\n\n# Test dataset (no augmentation)\ntest_generator = test_val_datagen.flow_from_dataframe(\n    test_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=False\n)\n\ntrain_generator\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\n\nclass_indices = train_generator.classes  \nclass_labels = list(train_generator.class_indices.keys())  \n\nclass_counts = Counter(class_indices)\n\nfor class_index, count in class_counts.items():\n    print(f\"Class '{class_labels[class_index]}' has {count} samples\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_size = (224,224)\nbatch_size = 32\ndatagen = ImageDataGenerator(\n    preprocessing_function= tf.keras.applications.vgg16.preprocess_input,\n    rescale=1./255,\n    horizontal_flip=True\n)\ntrain_generator = datagen.flow_from_dataframe(\n    train_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True\n)\ntest_generator = datagen.flow_from_dataframe(\n    test_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=False\n)\nval_generator = datagen.flow_from_dataframe(\n    validation_df,\n    x_col='image',\n    y_col='label',\n    color_mode='rgb',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\ntrain_generator","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_num=list(train_generator.class_indices.keys())\nclass_num","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**first model**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n    layers.BatchNormalization(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.5),\n\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    \n    layers.Dense(5, activation='softmax')\n])\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**second model**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    # Input Layer\n    layers.Input(shape=(224, 224, 3)),\n\n    # First Convolutional Block\n    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    # Second Convolutional Block\n    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    # Third Convolutional Block\n    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    # Fourth Convolutional Block\n    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.5),\n\n    # Fifth Convolutional Block (Additional Complexity)\n    layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.GlobalAveragePooling2D(),  # Replaces Flatten for better spatial aggregation\n    layers.Dropout(0.5),\n\n    # Fully Connected Layers\n    layers.Dense(1024, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n\n    # Output Layer\n    layers.Dense(5, activation='softmax')\n])\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True,show_dtype=True,dpi=120)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_cb =EarlyStopping(patience=10, restore_best_weights=True)\nmodel.compile(optimizer ='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=50, validation_data=val_generator, callbacks=[early_stopping_cb])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss,accuracy=model.evaluate(test_generator)\nprint(\"loss : \",loss)\nprint(\"accuracy : \",accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ef=pd.DataFrame(history.history)\nef[['loss','val_loss']].plot()\nef[['accuracy','val_accuracy']].plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ef=pd.DataFrame(history.history)\nef[['loss','val_loss']].plot()\nef[['accuracy','val_accuracy']].plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3D Convolutional Neural Network with Dual Attention Module**","metadata":{}},{"cell_type":"code","source":"class ChannelAttentionModule(layers.Layer):\n    def __init__(self, filters, reduction_ratio=8):\n        super(ChannelAttentionModule, self).__init__()\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n\n    def build(self, input_shape):\n        # shared MLP (Multi-Layer Perceptron)\n        self.mlp = models.Sequential([\n            layers.Dense(self.filters // self.reduction_ratio, activation='relu'),\n            layers.Dense(self.filters, activation=None)\n        ])\n\n    def call(self, inputs):\n        # global Average Pooling\n        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        avg_pool = self.mlp(avg_pool)\n\n        # global Max Pooling\n        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        max_pool = self.mlp(max_pool)\n\n        # combine Avg and Max Pooling\n        channel_attention = tf.sigmoid(avg_pool + max_pool)\n\n        # apply Channel Attention\n        output = inputs * channel_attention\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SpatialAttentionModule(layers.Layer):\n    def __init__(self):\n        super(SpatialAttentionModule, self).__init__()\n\n    def build(self, input_shape):\n        # convolutional layer to generate spatial attention map\n        self.conv = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n\n    def call(self, inputs):\n        # average Pooling along the channel axis\n        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n\n        # max Pooling along the channel axis\n        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n\n        # concatenate Avg and Max Pooling\n        concat = tf.concat([avg_pool, max_pool], axis=-1)\n\n        # generate Spatial Attention Map\n        spatial_attention = self.conv(concat)\n\n        # apply Spatial Attention\n        output = inputs * spatial_attention\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualAttentionModule2D(layers.Layer):\n    def __init__(self, filters, reduction_ratio=8):\n        super(DualAttentionModule2D, self).__init__()\n        self.channel_attention = ChannelAttentionModule(filters, reduction_ratio)\n        self.spatial_attention = SpatialAttentionModule()\n\n    def call(self, inputs):\n        # apply Channel Attention\n        x = self.channel_attention(inputs)\n\n        # apply Spatial Attention\n        x = self.spatial_attention(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualAttentionModule2D(layers.Layer):\n    def __init__(self, filters):\n        super(DualAttentionModule2D, self).__init__()\n        self.filters = filters\n\n    def build(self, input_shape):\n        # spatial Attention\n        self.spatial_attention = layers.Conv2D(1, kernel_size=1, activation='sigmoid')\n\n        # channel Attention\n        self.channel_attention = layers.GlobalAveragePooling2D()\n        self.channel_fc1 = layers.Dense(self.filters // 8, activation='relu')\n        self.channel_fc2 = layers.Dense(self.filters, activation='sigmoid')\n\n    def call(self, inputs):\n        # spatial Attention\n        spatial_attention_map = self.spatial_attention(inputs)\n        spatial_output = inputs * spatial_attention_map\n\n        # channel Attention\n        channel_attention_map = self.channel_attention(inputs)\n        channel_attention_map = self.channel_fc1(channel_attention_map)\n        channel_attention_map = self.channel_fc2(channel_attention_map)\n        channel_attention_map = tf.reshape(channel_attention_map, [-1, 1, 1, self.filters])\n        channel_output = inputs * channel_attention_map\n\n        # combine Spatial and Channel Attention\n        output = spatial_output + channel_output\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def residual_block_2d(x, filters, kernel_size=3, stride=1):\n    # Shortcut connection\n    shortcut = x\n\n    # First 2D Convolution\n    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    # Second 2D Convolution\n    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Add shortcut connection\n    if shortcut.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.Add()([x, shortcut])\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_2d_dam_model(input_shape=(224, 224, 3), num_classes=5):\n    inputs = layers.Input(shape=input_shape)\n\n    # Initial 2D Convolution\n    x = layers.Conv2D(8, kernel_size=3, padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    # Residual Blocks with Dual Attention Modules\n    filters_list = [16, 32, 64]\n    for filters in filters_list:\n        x = residual_block_2d(x, filters)\n        x = DualAttentionModule2D(filters)(x)\n\n    # Global Average Pooling\n    x = layers.GlobalAveragePooling2D()(x)\n\n    # Fully Connected Layers\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n\n    # Output Layer\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = models.Model(inputs, outputs)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = build_2d_dam_model(input_shape=(224, 224, 3), num_classes=5)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True,show_dtype=True,dpi=120)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_cb =EarlyStopping(patience=10, restore_best_weights=True)\nmodel.compile(optimizer ='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=50, validation_data=val_generator, callbacks=[early_stopping_cb])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}