{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input/alzheimers-adni","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport keras \nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import SMOTE\nfrom tensorflow.keras import layers, models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**preparing data**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\nimages = []\nlabels = []\n\ntrain_path = \"/kaggle/input/alzheimers-adni/Alzheimers-ADNI/train\"\n\nfor subfolder in tqdm(os.listdir(train_path)):\n    subfolder_path = os.path.join(train_path, subfolder)\n    \n    \n    if os.path.isfile(subfolder_path):\n        images.append(subfolder_path)\n        labels.append(subfolder)  \n        continue\n    \n    \n    for folder in os.listdir(subfolder_path):\n        subfolder_path2 = os.path.join(subfolder_path, folder)\n\n        if os.path.isfile(subfolder_path2):  \n           \n            images.append(subfolder_path2)\n            labels.append(subfolder)  \n            continue\n\n        \n        if os.path.isdir(subfolder_path2):\n            for image_filename in os.listdir(subfolder_path2):\n                image_path = os.path.join(subfolder_path2, image_filename)\n                if os.path.isfile(image_path):  \n                    images.append(image_path)\n                    labels.append(folder)  \n\ntrain_df = pd.DataFrame({'image': images, 'label': labels})\ntrain_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\n\nimages = []\nlabels = []\n\ntrain_path = \"/kaggle/input/alzheimers-adni/Alzheimers-ADNI/test\"\n\nfor subfolder in tqdm(os.listdir(train_path)):\n    subfolder_path = os.path.join(train_path, subfolder)\n    \n    \n    if os.path.isfile(subfolder_path):\n        images.append(subfolder_path)\n        labels.append(subfolder)  \n        continue\n    \n    \n    for folder in os.listdir(subfolder_path):\n        subfolder_path2 = os.path.join(subfolder_path, folder)\n\n        if os.path.isfile(subfolder_path2):  \n            \n            images.append(subfolder_path2)\n            labels.append(subfolder)  \n            continue\n\n        \n        if os.path.isdir(subfolder_path2):\n            for image_filename in os.listdir(subfolder_path2):\n                image_path = os.path.join(subfolder_path2, image_filename)\n                if os.path.isfile(image_path):  \n                    images.append(image_path)\n                    labels.append(folder)  \n\n\ntest_df = pd.DataFrame({'image': images, 'label': labels})\n\n\ntest_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# split into 50% test and 50% validation\nsplit_index = len(test_df) // 2\nvalidation_df = test_df.iloc[:split_index].reset_index(drop=True)\ntest_df = test_df.iloc[split_index:].reset_index(drop=True)\n\n\nprint(f\"Test Set: {len(test_df)} samples\")\nprint(f\"Validation Set: {len(validation_df)} samples\")\n\nprint(test_df)\nprint(validation_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert categorical labels to integers\nlabel_map = {'Final AD JPEG': 0, 'Final CN JPEG': 1, 'Final EMCI JPEG': 2, 'Final LMCI JPEG': 3, 'Final MCI JPEG': 4}\ntrain_df['label'] = train_df['label'].map(label_map)\nvalidation_df['label'] = validation_df['label'].map(label_map)\ntest_df['label'] = test_df['label'].map(label_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['label'] = train_df['label'].astype(int)\nvalidation_df['label'] = validation_df['label'].astype(int)\ntest_df['label'] = test_df['label'].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\n\nclass BrainDataset(Dataset):\n    def __init__(self, dataframe, image_size=(224, 224), transform=None):\n        self.dataframe = dataframe\n        self.image_size = image_size\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['image']\n        label = self.dataframe.iloc[idx]['label']\n\n        \n        image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n        image = image.resize(self.image_size)\n\n        if self.transform:\n            image = self.transform(image)\n \n\n        # Convert label to a tensor\n        label = torch.tensor(label, dtype=torch.long)\n\n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to 224x224\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = BrainDataset(train_df, transform=transform)\nval_dataset = BrainDataset(validation_df, transform=transform)\ntest_dataset = BrainDataset(test_df, transform=transform)\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Implementing ResNet-based Model with a Bias-aware Pruning Technique**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Define the 3x3 convolution function\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n# Define the BasicBlock\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, cfg=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, cfg, stride)\n        self.bn1 = nn.BatchNorm2d(cfg)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(cfg, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu(out)\n        return out\n\n# Define the ResNet model for brain images\nclass ResNet_Brain(nn.Module):\n    def __init__(self, block, layers, cfg=None, num_classes=5):\n        super(ResNet_Brain, self).__init__()\n        self.inplanes = 64  # Increased initial channels for brain images\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  \n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        # Residual layers\n        self.layer1 = self._make_layer(block, 64, layers[0], cfg=cfg[0:layers[0]])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, cfg=cfg[layers[0]:layers[0] + layers[1]])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, cfg=cfg[layers[0] + layers[1]:layers[0] + layers[1] + layers[2]])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, cfg=cfg[layers[0] + layers[1] + layers[2]:layers[0] + layers[1] + layers[2] + layers[3]])\n        \n        # Global average pooling and fully connected layer\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling for variable input sizes\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1, cfg=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, cfg=cfg[0]))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cfg=cfg[i]))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n# Define a function to create the ResNet model for brain images\ndef resnet18_brain(cfg=None, num_classes=5):\n    return ResNet_Brain(BasicBlock, [2, 2, 2, 2], cfg=cfg, num_classes=num_classes)\n\n\ncfg = [64] * 8 + [128] * 8 + [256] * 8 + [512] * 8  # Example configuration\nmodel = resnet18_brain(cfg=cfg, num_classes=5)\nprint(model)\n\n\n#input_tensor = torch.randn(1, 3, 224, 224)  \n#output = model(input_tensor)\n#print(output.shape)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()  # Loss function for classification\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n\n\n# Define the device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#spliting bias-aligning and bias-conflicting samples\ndef get_S_bc_S_ba2(train_loader, model, device):\n    S_bc = set()\n    S_ba = set()\n    unknown = set()\n\n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(train_loader):  # Unpack only images and labels\n            images, labels = images.to(device), labels.to(device)\n            logits = model(images)\n            outputs = torch.sigmoid(logits)\n            predicted_labels = outputs.argmax(dim=1)\n\n            for idx, label in enumerate(labels):\n                sample_index = batch_idx * train_loader.batch_size + idx  # Calculate sample index\n                if predicted_labels[idx] == label.item():\n                    S_ba.add(sample_index)\n                elif predicted_labels[idx] != label.item():\n                    S_bc.add(sample_index)\n                else:\n                    unknown.add(sample_index)\n\n    return S_bc, S_ba","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#bias calculation\ndef get_conv2d_output(model, S, train_loader, device):\n    # Initialize a dictionary to store the sum of filter activations for each layer\n    filter_activations_sum = {}\n\n    # Define the hook function to collect filter activations for each layer\n    def getActivation(layer_name):\n        def hook(self, input, output):\n            nonlocal filter_activations_sum\n            if layer_name in filter_activations_sum:\n                filter_activations_sum[layer_name] += output.mean(dim=(2, 3)).detach().squeeze()\n            else:\n                filter_activations_sum[layer_name] = output.mean(dim=(2, 3)).detach().squeeze()\n        return hook\n\n    hooks = []  # List to store hooks for later removal\n\n    # Attach hooks to the convolutional layers in each BasicBlock\n    for i in range(1, 5):  # Loop through layer1, layer2, layer3, layer4\n        layer = getattr(model, f'layer{i}')\n        for j, block in enumerate(layer):  # Loop through each BasicBlock in the layer\n            # Attach hook to conv1 in the BasicBlock\n            layer_name = f\"layer{i}_block{j}_conv1\"\n            handler = block.conv1.register_forward_hook(getActivation(layer_name))\n            hooks.append(handler)\n\n            # Attach hook to conv2 in the BasicBlock\n            layer_name = f\"layer{i}_block{j}_conv2\"\n            handler = block.conv2.register_forward_hook(getActivation(layer_name))\n            hooks.append(handler)\n\n    # Process samples in S\n    for sample_idx in S:\n        # Load an image and label from the dataset\n        image, label = train_loader.dataset[sample_idx]  # Assuming dataset returns (image, label)\n        image = image.to(device)\n        image = image.unsqueeze(0)  # Add batch dimension\n        # Forward pass through the model\n        with torch.no_grad():\n            output = model(image)\n\n    # Calculate the average filter activations for each layer\n    average_filter_activations = {layer: filter_activations_sum[layer] / len(S) for layer in filter_activations_sum}\n\n    # Remove all the hooks after the forward pass is completed\n    for hook in hooks:\n        hook.remove()\n\n    return average_filter_activations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\n# Define the device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nnum_epochs = 40\npatience = 5  # Number of epochs to wait for improvement before stopping\nbest_val_loss = np.inf  # Initialize best validation loss to infinity\nepochs_without_improvement = 0  # Counter for epochs without improvement\npruning_ratio = 0.1  # Prune 10% of the filters\nfinetune_epochs = 10  # Number of epochs for fine-tuning\n\n# Function to evaluate the model on the test set\ndef evaluate_test_accuracy(model, test_loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0.0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    test_accuracy = 100 * correct / total\n    test_loss = test_loss / len(test_loader)\n    return test_loss, test_accuracy\n\n# Early stopping function\ndef early_stopping(val_loss, patience, epochs_without_improvement, best_val_loss):\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        epochs_without_improvement = 0\n        # Save the best model\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement >= patience:\n            print(f\"Early stopping triggered after {epochs_without_improvement} epochs without improvement.\")\n            return True, best_val_loss, epochs_without_improvement\n    return False, best_val_loss, epochs_without_improvement\n\nmodel = model.to(device)\n# Training loop with early stopping\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Training phase\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)  # Move data to the device\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Compute accuracy\n        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    train_accuracy = 100 * correct / total\n    train_loss = running_loss / len(train_loader)\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)  # Move data to the device\n            outputs = model(images)\n            val_loss = criterion(outputs, labels)\n            val_running_loss += val_loss.item()\n\n            # Compute accuracy\n            _, predicted = torch.max(outputs, 1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_accuracy = 100 * val_correct / val_total\n    val_loss = val_running_loss / len(val_loader)\n\n    # Test evaluation phase\n    test_loss, test_accuracy = evaluate_test_accuracy(model, test_loader, device)\n\n    # Print metrics\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, \"\n          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    # Early stopping check\n    stop_training, best_val_loss, epochs_without_improvement = early_stopping(\n        val_loss, patience, epochs_without_improvement, best_val_loss\n    )\n    if stop_training:\n        break\n\nprint(\"Training complete. Best model saved to 'best_model.pth'.\")\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.to(device)  # Move the model to the device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\n# Define the device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Function to evaluate the model on the test set\ndef evaluate_test_accuracy(model, test_loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0.0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    test_accuracy = 100 * correct / total\n    test_loss = test_loss / len(test_loader)\n    return test_loss, test_accuracy\n\n# Define the function to copy matching weights\ndef copy_matching_weights(pruned_model, original_model):\n    pruned_state_dict = pruned_model.state_dict()\n    original_state_dict = original_model.state_dict()\n\n    for name, param in original_state_dict.items():\n        if name in pruned_state_dict:\n            if pruned_state_dict[name].shape == param.shape:\n                pruned_state_dict[name].copy_(param)\n            else:\n                print(f\"Skipping {name} due to shape mismatch: {param.shape} vs {pruned_state_dict[name].shape}\")\n        else:\n            print(f\"Skipping {name} as it is not in the pruned model\")\n\n    pruned_model.load_state_dict(pruned_state_dict, strict=False)\n\n# Pruning Phase\nprint(\"Starting Pruning...\")\nmodel = model.to(device)\npruning_ratio = 0.5  # Prune 50% of the filters\n\n# Get bias-aligned and bias-conflicting samples\nS_bc, S_ba = get_S_bc_S_ba2(train_loader, model, device)\n\n# Calculate bias scores\nbias_alligned = get_conv2d_output(model, S_ba, train_loader, device)\nbias_conflicting = get_conv2d_output(model, S_bc, train_loader, device)\nbias_score = {key: bias_alligned[key] - bias_conflicting[key] for key in bias_alligned}\n\n# Calculate threshold for pruning\ntotal_channel = sum([n.shape[0] for n in bias_score.values()])\nfeature_s = torch.cat([n.flatten() for n in bias_score.values()])\ny, i = torch.sort(feature_s, descending=True)\nthre_index = int(total_channel * pruning_ratio)\nthre = y[thre_index]\n\n# Prune the model\ncfg1 = []\ncfg_mask = []\npruned = 0\nfor i, feature_copy in enumerate(bias_score.values()):\n    mask = feature_copy.gt(thre).float()  # Create a mask for pruning\n    if torch.sum(mask) == 0:\n        cfg1.append(len(feature_copy))\n        cfg_mask.append(torch.ones(len(feature_copy)).float())\n    else:\n        pruned += mask.shape[0] - torch.sum(mask)\n        cfg1.append(int(torch.sum(mask)))\n        cfg_mask.append(mask.clone())\n\n# Save the pruned model\npruned_model = resnet18_brain(cfg=cfg1)  # Replace with your model class\ncopy_matching_weights(pruned_model, model)  # Copy matching weights\npruned_model.to(device)  # Move the pruned model to the device\n\n# Evaluate test accuracy after pruning\ntest_loss, test_accuracy = evaluate_test_accuracy(pruned_model, test_loader, device)\nprint(f\"Test Accuracy after Pruning: {test_accuracy:.2f}%\")\n\ntorch.save({'cfg': cfg1, 'state_dict': pruned_model.state_dict()}, 'pruned_model.pth.tar')\nprint(\"Pruning complete. Pruned model saved to 'pruned_model.pth.tar'.\")\n\n# Fine-Tuning Phase\nprint(\"Starting Fine-Tuning...\")\npruned_model.load_state_dict(torch.load('pruned_model.pth.tar')['state_dict'])\npruned_model.to(device)\n\n# Define loss and optimizer for fine-tuning\ntune_criterion = nn.CrossEntropyLoss()  # Use standard cross-entropy loss for fine-tuning\nfinetune_optimizer = torch.optim.Adam(pruned_model.parameters(), lr=1e-4, weight_decay=5e-4)\n\n# Fine-tuning loop\nfinetune_epochs = 10  # Number of epochs for fine-tuning\nfor epoch in range(finetune_epochs):\n    pruned_model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = pruned_model(images)\n        loss = tune_criterion(outputs, labels)\n\n        finetune_optimizer.zero_grad()\n        loss.backward()\n        finetune_optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    train_accuracy = 100 * correct / total\n    train_loss = running_loss / len(train_loader)\n\n    # Evaluate test accuracy after each fine-tuning epoch\n    test_loss, test_accuracy = evaluate_test_accuracy(pruned_model, test_loader, device)\n    print(f\"Fine-Tuning Epoch [{epoch + 1}/{finetune_epochs}], \"\n          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n# Save the fine-tuned model\ntorch.save({\n    'cfg': cfg1,  # Save the configuration\n    'state_dict': pruned_model.state_dict(),  # Save the model's state_dict\n}, 'fine_tuned_model.pth')\nprint(\"Fine-Tuning complete. Fine-tuned model saved to 'fine_tuned_model.pth'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Preparing data for other models**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\n\nimage_size = (224, 224)\nbatch_size = 32\n\n# function to add Gaussian noise\ndef add_gaussian_noise(image):\n    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.05, dtype=tf.float32)\n    return tf.clip_by_value(image + noise, 0.0, 1.0)  # Ensures values remain between [0,1]\n\n# data augmentation for training set\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=lambda x: add_gaussian_noise(tf.keras.applications.vgg16.preprocess_input(x)),\n    rescale=1./255,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2]\n)\n\n\ntest_val_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n    rescale=1./255\n)\n\n# Train dataset with augmentation\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True\n)\n\n# Validation dataset (no augmentation)\nval_generator = test_val_datagen.flow_from_dataframe(\n    validation_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True\n)\n\n# Test dataset (no augmentation)\ntest_generator = test_val_datagen.flow_from_dataframe(\n    test_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=False\n)\n\ntrain_generator","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\n\nclass_indices = train_generator.classes  \nclass_labels = list(train_generator.class_indices.keys())  \n\nclass_counts = Counter(class_indices)\n\nfor class_index, count in class_counts.items():\n    print(f\"Class '{class_labels[class_index]}' has {count} samples\")\n\nimage_size = (224,224)\nbatch_size = 32\ndatagen = ImageDataGenerator(\n    preprocessing_function= tf.keras.applications.vgg16.preprocess_input,\n    rescale=1./255,\n    horizontal_flip=True\n)\ntrain_generator = datagen.flow_from_dataframe(\n    train_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=True\n)\ntest_generator = datagen.flow_from_dataframe(\n    test_df,\n    x_col='image',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode='rgb',\n    shuffle=False\n)\nval_generator = datagen.flow_from_dataframe(\n    validation_df,\n    x_col='image',\n    y_col='label',\n    color_mode='rgb',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\ntrain_generator\n\nclass_num=list(train_generator.class_indices.keys())\nclass_num","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**baseline CNN Model**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n    layers.BatchNormalization(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.5),\n\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    \n    layers.Dense(5, activation='softmax')\n])\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True,show_dtype=True,dpi=120)\n\nearly_stopping_cb =EarlyStopping(patience=10, restore_best_weights=True)\nmodel.compile(optimizer ='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=50, validation_data=val_generator, callbacks=[early_stopping_cb])\n\nloss,accuracy=model.evaluate(test_generator)\nprint(\"loss : \",loss)\nprint(\"accuracy : \",accuracy)\n\nef=pd.DataFrame(history.history)\nef[['loss','val_loss']].plot()\nef[['accuracy','val_accuracy']].plot()\n\nef=pd.DataFrame(history.history)\nef[['loss','val_loss']].plot()\nef[['accuracy','val_accuracy']].plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2D Convolutional Neural Network with Dual Attention Module**","metadata":{}},{"cell_type":"code","source":"class ChannelAttentionModule(layers.Layer):\n    def __init__(self, filters, reduction_ratio=8):\n        super(ChannelAttentionModule, self).__init__()\n        self.filters = filters\n        self.reduction_ratio = reduction_ratio\n\n    def build(self, input_shape):\n        # shared MLP (Multi-Layer Perceptron)\n        self.mlp = models.Sequential([\n            layers.Dense(self.filters // self.reduction_ratio, activation='relu'),\n            layers.Dense(self.filters, activation=None)\n        ])\n\n    def call(self, inputs):\n        # global Average Pooling\n        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        avg_pool = self.mlp(avg_pool)\n\n        # global Max Pooling\n        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        max_pool = self.mlp(max_pool)\n\n        # combine Avg and Max Pooling\n        channel_attention = tf.sigmoid(avg_pool + max_pool)\n\n        # apply Channel Attention\n        output = inputs * channel_attention\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SpatialAttentionModule(layers.Layer):\n    def __init__(self):\n        super(SpatialAttentionModule, self).__init__()\n\n    def build(self, input_shape):\n        # convolutional layer to generate spatial attention map\n        self.conv = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n\n    def call(self, inputs):\n        # average Pooling along the channel axis\n        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n\n        # max Pooling along the channel axis\n        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n\n        # concatenate Avg and Max Pooling\n        concat = tf.concat([avg_pool, max_pool], axis=-1)\n\n        # generate Spatial Attention Map\n        spatial_attention = self.conv(concat)\n\n        # apply Spatial Attention\n        output = inputs * spatial_attention\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualAttentionModule2D(layers.Layer):\n    def __init__(self, filters, reduction_ratio=8):\n        super(DualAttentionModule2D, self).__init__()\n        self.channel_attention = ChannelAttentionModule(filters, reduction_ratio)\n        self.spatial_attention = SpatialAttentionModule()\n\n    def call(self, inputs):\n        # apply Channel Attention\n        x = self.channel_attention(inputs)\n\n        # apply Spatial Attention\n        x = self.spatial_attention(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualAttentionModule2D(layers.Layer):\n    def __init__(self, filters):\n        super(DualAttentionModule2D, self).__init__()\n        self.filters = filters\n\n    def build(self, input_shape):\n        # spatial Attention\n        self.spatial_attention = layers.Conv2D(1, kernel_size=1, activation='sigmoid')\n\n        # channel Attention\n        self.channel_attention = layers.GlobalAveragePooling2D()\n        self.channel_fc1 = layers.Dense(self.filters // 8, activation='relu')\n        self.channel_fc2 = layers.Dense(self.filters, activation='sigmoid')\n\n    def call(self, inputs):\n        # spatial Attention\n        spatial_attention_map = self.spatial_attention(inputs)\n        spatial_output = inputs * spatial_attention_map\n\n        # channel Attention\n        channel_attention_map = self.channel_attention(inputs)\n        channel_attention_map = self.channel_fc1(channel_attention_map)\n        channel_attention_map = self.channel_fc2(channel_attention_map)\n        channel_attention_map = tf.reshape(channel_attention_map, [-1, 1, 1, self.filters])\n        channel_output = inputs * channel_attention_map\n\n        # combine Spatial and Channel Attention\n        output = spatial_output + channel_output\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def residual_block_2d(x, filters, kernel_size=3, stride=1):\n    # Shortcut connection\n    shortcut = x\n\n    # First 2D Convolution\n    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    # Second 2D Convolution\n    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    # Add shortcut connection\n    if shortcut.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.Add()([x, shortcut])\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_2d_dam_model(input_shape=(224, 224, 3), num_classes=5):\n    inputs = layers.Input(shape=input_shape)\n\n    # Initial 2D Convolution\n    x = layers.Conv2D(8, kernel_size=3, padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    # Residual Blocks with Dual Attention Modules\n    filters_list = [16, 32, 64]\n    for filters in filters_list:\n        x = residual_block_2d(x, filters)\n        x = DualAttentionModule2D(filters)(x)\n\n    # Global Average Pooling\n    x = layers.GlobalAveragePooling2D()(x)\n\n    # Fully Connected Layers\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n\n    # Output Layer\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = models.Model(inputs, outputs)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = build_2d_dam_model(input_shape=(224, 224, 3), num_classes=5)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\ntf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True,show_dtype=True,dpi=120)\n\nearly_stopping_cb =EarlyStopping(patience=10, restore_best_weights=True)\nmodel.compile(optimizer ='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=50, validation_data=val_generator, callbacks=[early_stopping_cb])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}